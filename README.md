# Prediction Agent - RL + MCP Integration

A lightweight prediction agent that ingests public market data, uses Reinforcement Learning to produce ranked trading signals, and exposes MCP-style endpoints with secure API access.

**Optimized for edge deployment:** RTX 3060 + 16-32GB RAM

‚ö†Ô∏è **IMPORTANT:** This agent produces predictions only. Execution/placing trades is out of scope.

---

## üìã Table of Contents

- [Features](#features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [API Usage](#api-usage)
- [Training](#training)
- [Configuration](#configuration)
- [RL Primer](#rl-primer)
- [Evaluation Checklist](#evaluation-checklist)

---

## ‚ú® Features

- **Multi-Source Data Ingestion**: Yahoo Finance, CoinGecko, Quandl support
- **Comprehensive Feature Engineering**: 50+ technical indicators (SMA, EMA, RSI, MACD, Bollinger Bands, etc.)
- **Dual Model Architecture**:
  - Baseline LightGBM for fast predictions
  - RL agents for ranking (LinUCB, Thompson Sampling, or DQN)
- **MCP-Compatible API**: JSON-RPC style endpoints for easy integration
- **JWT Authentication**: Secure API access
- **Rate Limiting**: Protection against abuse
- **Request Logging**: Complete audit trail
- **Risk-Aware**: Dynamic stop-loss, capital risk, and drawdown parameters
- **Edge-Optimized**: Runs efficiently on RTX 3060 with 16-32GB RAM

---

## üèóÔ∏è Architecture

```
prediction-agent-test/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ data_ingest.py          # Multi-source data fetching with caching
‚îÇ   ‚îú‚îÄ‚îÄ features.py             # Feature engineering pipeline
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline_lightgbm.py  # Fast tree-based predictor
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rl_agent.py         # RL agents (LinUCB/Thompson/DQN)
‚îÇ   ‚îî‚îÄ‚îÄ mcp_adapter.py          # MCP protocol wrapper
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îî‚îÄ‚îÄ server.py               # FastAPI server with JWT auth
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îú‚îÄ‚îÄ data_demo.ipynb         # Data ingestion demo
‚îÇ   ‚îî‚îÄ‚îÄ rl_demo.ipynb           # RL training demo
‚îú‚îÄ‚îÄ tests/                      # Unit tests
‚îú‚îÄ‚îÄ logs/                       # Request logs and shortlists
‚îú‚îÄ‚îÄ models/                     # Saved model artifacts
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ cache/                  # Cached market data (Parquet)
‚îÇ   ‚îî‚îÄ‚îÄ features/               # Feature store
‚îú‚îÄ‚îÄ .env.template               # Environment variables template
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ README.md                   # This file
‚îî‚îÄ‚îÄ REFLECTION.md              # Humility/Gratitude/Honesty reflection
```

---

## üöÄ Installation

### Prerequisites

- Python 3.8+
- CUDA-capable GPU (optional, for DQN acceleration)
- 16-32GB RAM
- RTX 3060 or better (for full pre-market scans)

### Step 1: Clone and Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd prediction-agent-test

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Step 2: Install TA-Lib (Required for technical indicators)

**Linux/Mac:**
```bash
wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz
tar -xzf ta-lib-0.4.0-src.tar.gz
cd ta-lib/
./configure --prefix=/usr
make
sudo make install
pip install ta-lib
```

**Windows:**
Download pre-built wheel from: https://www.lfd.uci.edu/~gohlke/pythonlibs/#ta-lib
```bash
pip install TA_Lib‚Äë0.4.xx‚Äëcpxx‚Äëcpxxm‚Äëwin_amd64.whl
```

### Step 3: Configure Environment

```bash
# Copy environment template
cp .env.template .env

# Edit .env and add your configurations
# IMPORTANT: Generate secure JWT secret key
openssl rand -hex 32  # Copy output to JWT_SECRET_KEY in .env

# Optional: Add API keys for additional data sources
# - QUANDL_API_KEY: Get from https://data.nasdaq.com/sign-up
# - COINGECKO_API_KEY: Free tier needs no key
```

---

## ‚ö° Quick Start

### 1. Ingest Data

```bash
# Run data ingestion script
python core/data_ingest.py
```

This will:
- Fetch data from Yahoo Finance for default universe
- Cache data locally in `data/cache/`
- Handle rate limits automatically

**Customize symbols:**
Create a `universe.txt` file with one symbol per line:
```txt
AAPL
MSFT
GOOGL
TSLA
RELIANCE.NS
TCS.NS
```

### 2. Generate Features

```bash
python -c "
from core.data_ingest import DataIngestion
from core.features import FeaturePipeline

# Ingest data
ingestor = DataIngestion()
raw_data = ingestor.get_universe_data()

# Generate features
pipeline = FeaturePipeline()
feature_store = pipeline.process_universe(raw_data, save=True)
print(f'‚úì Processed {len(feature_store)} symbols')
"
```

### 3. Train Baseline Model

```bash
python -c "
from core.features import FeaturePipeline
from core.models.baseline_lightgbm import BaselineLightGBM
import pandas as pd

# Load features
pipeline = FeaturePipeline()
feature_store = pipeline.load_feature_store()

# Combine all data
all_data = pd.concat(feature_store.values())
feature_cols = [col for col in all_data.columns if col not in 
                ['Open', 'High', 'Low', 'Close', 'Volume', 
                 'symbol', 'source', 'fetch_timestamp', 'target', 'target_direction']]

# Train model
model = BaselineLightGBM(task='classification')
X_train, X_test, y_train, y_test = model.prepare_data(
    all_data, feature_cols, 'target_direction'
)
model.train(X_train, y_train, X_test, y_test)
model.save()
print('‚úì Baseline model trained and saved')
"
```

### 4. Train RL Agent

```bash
python -c "
from core.features import FeaturePipeline
from core.models.rl_agent import LinUCBAgent, RLTrainer

# Load features
pipeline = FeaturePipeline()
feature_store = pipeline.load_feature_store()

# Initialize agent
agent = LinUCBAgent(n_features=50, alpha=1.0)

# Train
trainer = RLTrainer(agent, feature_store, agent_type='linucb')
stats = trainer.train_bandit(n_rounds=100, top_k=20)

# Save agent
agent.save()
print(f'‚úì RL agent trained - Avg Reward: {stats[\"avg_reward\"]:.4f}')
"
```

### 5. Start API Server

```bash
# Start server
python api/server.py

# Or use uvicorn directly
uvicorn api.server:app --host 0.0.0.0 --port 8000 --reload
```

Server will start at: `http://localhost:8000`
API docs available at: `http://localhost:8000/docs`

---

## üì° API Usage

### Authentication

First, get a JWT token:

```bash
curl -X POST "http://localhost:8000/auth/token" \
  -H "Content-Type: application/json" \
  -d '{
    "username": "user",
    "password": "password"
  }'
```

Response:
```json
{
  "access_token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9...",
  "token_type": "bearer",
  "expires_in": 1800
}
```

### 1. Health Check

```bash
curl -X GET "http://localhost:8000/tools/health"
```

Response:
```json
{
  "status": "healthy",
  "timestamp": "2025-10-01T10:00:00Z",
  "uptime_seconds": 3600.5,
  "system": {
    "cpu_percent": 25.3,
    "memory_percent": 45.2,
    "memory_available_mb": 8192.5,
    "gpu": {
      "available": true,
      "device_name": "NVIDIA GeForce RTX 3060",
      "memory_allocated_mb": 512.3,
      "memory_reserved_mb": 1024.0
    },
    "request_count": 150
  },
  "models_loaded": true
}
```

### 2. Predict Specific Symbols

```bash
curl -X POST "http://localhost:8000/tools/predict" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE" \
  -H "Content-Type: application/json" \
  -d '{
    "symbols": ["AAPL", "MSFT", "GOOGL"],
    "horizon": "intraday",
    "risk_profile": {
      "stop_loss_pct": 2.0,
      "capital_risk_pct": 1.5,
      "drawdown_limit_pct": 10.0
    }
  }'
```

Response:
```json
{
  "success": true,
  "data": [
    {
      "symbol": "AAPL",
      "horizon": "intraday",
      "predicted_price": 178.45,
      "confidence": 0.87,
      "score": 0.91,
      "action": "long",
      "risk_applied": {
        "stop_loss_pct": 2.0,
        "capital_risk_pct": 1.5,
        "drawdown_limit_pct": 10.0
      },
      "reason": "Momentum + volume breakout",
      "timestamp": "2025-10-01T10:00:00Z",
      "model_version": "lightgbm-v1+linucbagent-v1"
    }
  ],
  "metadata": {
    "total_predictions": 3,
    "requested_symbols": 3,
    "horizon": "intraday"
  },
  "error": null
}
```

### 3. Scan All Symbols (Pre-Market Scan)

```bash
curl -X POST "http://localhost:8000/tools/scan_all" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE" \
  -H "Content-Type: application/json" \
  -d '{
    "horizon": "daily",
    "top_k": 10,
    "min_score": 0.6,
    "risk_profile": {
      "stop_loss_pct": 2.0,
      "capital_risk_pct": 1.5
    }
  }'
```

This endpoint:
- Scans entire cached universe
- Ranks by RL score
- Returns top K symbols
- Saves results to `logs/shortlist_YYYYMMDD_HHMMSS.json`

### 4. Analyze Symbols

```bash
curl -X POST "http://localhost:8000/tools/analyze" \
  -H "Authorization: Bearer YOUR_TOKEN_HERE" \
  -H "Content-Type: application/json" \
  -d '{
    "symbols": ["AAPL", "TSLA"],
    "horizon": "daily",
    "detailed": true
  }'
```

Detailed analysis includes:
- Technical indicators (RSI, MACD, SMA, etc.)
- Recent performance (1d, 5d, 20d returns)
- RL prediction scores

---

## üéì RL Primer

### What is Reinforcement Learning?

Reinforcement Learning (RL) is a machine learning paradigm where agents learn to make decisions by interacting with an environment and receiving rewards.

**Key Concepts:**
- **State**: Current market conditions (features)
- **Action**: Trading decision (long, short, hold)
- **Reward**: Profit/loss from the action
- **Policy**: Strategy for selecting actions

### Agent Types in This Project

#### 1. LinUCB (Linear Upper Confidence Bound)

**Best for:** Fast, efficient ranking with uncertainty estimates

**How it works:**
- Maintains a linear model for each symbol (arm)
- Balances exploration (trying new symbols) vs exploitation (using known good symbols)
- Uses UCB formula: `score = Œ∏·µÄx + Œ±‚àö(x·µÄA‚Åª¬πx)`
  - First term: expected reward
  - Second term: uncertainty bonus (exploration)

**When to use:**
- Quick pre-market scans
- Limited computational resources
- Need confidence estimates

**Parameters:**
- `alpha`: Exploration parameter (higher = more exploration)

#### 2. Thompson Sampling

**Best for:** Bayesian approach with posterior sampling

**How it works:**
- Maintains posterior distribution over reward parameters
- Samples from posterior to make decisions
- Naturally balances exploration/exploitation

**When to use:**
- Want probabilistic predictions
- Need diverse recommendations
- Prefer Bayesian framework

**Parameters:**
- `lambda_`: Regularization strength
- `v`: Noise variance

#### 3. DQN (Deep Q-Network)

**Best for:** Complex state representations, non-linear patterns

**How it works:**
- Neural network approximates Q-values: `Q(s,a)` = expected future reward
- Experience replay: stores and samples past experiences
- Target network: stabilizes training

**When to use:**
- Complex feature interactions
- Have GPU available
- Need multi-step planning

**Parameters:**
- `epsilon`: Exploration rate (decays over time)
- `gamma`: Discount factor (0.99 = care about future)
- `learning_rate`: Step size for updates

### Reward Design

For trading, we use **simulated PnL** as reward:

```python
# For long position
reward = future_return * 100

# For short position  
reward = -future_return * 100

# For hold
reward = 0
```

This aligns agent objectives with profitable predictions.

### Training Tips

1. **Start simple**: LinUCB ‚Üí Thompson ‚Üí DQN
2. **Use short horizons**: 1-5 day predictions work best
3. **Monitor cumulative reward**: Should increase over training
4. **Check Sharpe ratio**: `mean_return / std_return * ‚àö252`
5. **Avoid overfitting**: Use time-series splits, not random

---

## üß™ Training

### Full Training Pipeline

```bash
python -c "
import logging
from core.data_ingest import DataIngestion
from core.features import FeaturePipeline
from core.models.baseline_lightgbm import BaselineLightGBM
from core.models.rl_agent import LinUCBAgent, RLTrainer
import pandas as pd

logging.basicConfig(level=logging.INFO)

# Step 1: Data Ingestion
print('Step 1: Ingesting data...')
ingestor = DataIngestion()
raw_data = ingestor.get_universe_data()

# Step 2: Feature Engineering
print('Step 2: Engineering features...')
pipeline = FeaturePipeline()
feature_store = pipeline.process_universe(raw_data, save=True)

# Step 3: Train Baseline
print('Step 3: Training baseline model...')
all_data = pd.concat(feature_store.values())
feature_cols = [col for col in all_data.columns if col not in 
                ['Open', 'High', 'Low', 'Close', 'Volume', 
                 'symbol', 'source', 'fetch_timestamp', 'target', 'target_direction']]

baseline = BaselineLightGBM(task='classification')
X_train, X_test, y_train, y_test = baseline.prepare_data(
    all_data, feature_cols, 'target_direction'
)
baseline.train(X_train, y_train, X_test, y_test, num_boost_round=100)
metrics = baseline.evaluate(X_test, y_test)
baseline.save()

# Step 4: Train RL Agent
print('Step 4: Training RL agent...')
agent = LinUCBAgent(n_features=len(feature_cols), alpha=1.0)
trainer = RLTrainer(agent, feature_store, agent_type='linucb')
stats = trainer.train_bandit(n_rounds=100, top_k=20, horizon=1)
agent.save()

# Step 5: Evaluate
print('Step 5: Evaluating...')
eval_metrics = trainer.evaluate(top_k=20)

print('\\n=== Training Complete ===')
print(f'Baseline AUC: {metrics[\"auc\"]:.4f}')
print(f'RL Avg Reward: {stats[\"avg_reward\"]:.4f}')
print(f'Sharpe Proxy: {eval_metrics[\"sharpe_proxy\"]:.4f}')
print(f'Win Rate: {eval_metrics[\"win_rate\"]:.2%}')
"
```

### Training for RTX 3060

**Memory Optimization Tips:**

1. **Batch Processing**: Process symbols in batches
```python
batch_size = 32
for i in range(0, len(symbols), batch_size):
    batch = symbols[i:i+batch_size]
    # Process batch
```

2. **Use ONNX Export**: For faster inference
```python
# Export LightGBM to ONNX (optional)
# Requires: pip install onnxmltools skl2onnx
```

3. **Multiprocessing**: Parallel feature computation
```python
from multiprocessing import Pool
with Pool(processes=4) as pool:
    results = pool.map(compute_features, symbols)
```

**Estimated Resource Usage (RTX 3060):**
- Full universe scan (100 symbols): ~2GB RAM, 30 seconds
- Model training: ~4GB RAM, 5-10 minutes
- Inference (single symbol): <100ms

---

## ‚öôÔ∏è Configuration

### Environment Variables (.env)

```bash
# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_DEBUG=False

# JWT Authentication
JWT_SECRET_KEY=<generate-with-openssl-rand-hex-32>
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30

# Data Sources
QUANDL_API_KEY=<your-key-here>  # Optional
COINGECKO_API_KEY=              # Free tier needs no key

# Paths
DATA_CACHE_DIR=./data/cache
FEATURE_STORE_DIR=./data/features
MODEL_DIR=./models
LOG_DIR=./logs

# Model Selection
RL_AGENT_TYPE=linucb  # Options: linucb, thompson, dqn
BASELINE_MODEL_NAME=lightgbm-v1

# Performance
USE_ONNX=False
ENABLE_MULTIPROCESSING=True
MAX_WORKERS=4
GPU_DEVICE=cuda:0  # Or cpu

# Rate Limiting
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=3600
```

---

## ‚úÖ Evaluation Checklist

### Data Authenticity
- [x] Uses real public sources (Yahoo Finance, CoinGecko)
- [x] No Kaggle-only datasets
- [x] Batched fetching with exponential backoff
- [x] Local caching (Parquet format)

### Functionality
- [x] All endpoints operational (`/tools/predict`, `/tools/scan_all`, `/tools/analyze`, `/tools/health`)
- [x] JWT authentication implemented
- [x] Rate limiting active
- [x] Request logging enabled

### RL Quality
- [x] Three agent implementations (LinUCB, Thompson Sampling, DQN)
- [x] Training loop with replay buffer
- [x] Evaluation metrics (cumulative reward, Sharpe proxy, win rate)
- [x] Checkpointing and model saving

### MCP Readiness
- [x] MCP adapter with clean request/response JSON
- [x] Tool conventions followed
- [x] Proper error handling
- [x] Request/response logging

### Edge Performance
- [x] Optimized for RTX 3060 + 16-32GB RAM
- [x] Multiprocessing support for parallel scoring
- [x] Memory-efficient data structures
- [x] Fast inference mode (<100ms per symbol)

### Security & Hygiene
- [x] No secrets in code (.env.template provided)
- [x] JWT authentication required for all tool endpoints
- [x] Input validation on all requests
- [x] Detailed logging to files

### Software Quality
- [x] Modular code structure
- [x] Comprehensive README
- [x] Type hints and documentation
- [x] Error handling throughout

---

## üìä Example Outputs

### Shortlist JSON Format

File: `logs/shortlist_20251001_100000.json`

```json
[
  {
    "symbol": "AAPL",
    "horizon": "intraday",
    "predicted_price": 178.45,
    "confidence": 0.87,
    "score": 0.91,
    "action": "long",
    "risk_applied": {
      "stop_loss_pct": 2.0,
      "capital_risk_pct": 1.5,
      "drawdown_limit_pct": 10.0
    },
    "reason": "Positive MACD + High volume",
    "timestamp": "2025-10-01T10:00:00Z",
    "model_version": "lightgbm-v1+linucbagent-v1"
  }
]
```

---

## üêõ Troubleshooting

### Issue: "Feature store not found"
**Solution:** Run data ingestion and feature generation first

### Issue: "Model not trained"
**Solution:** Train baseline and RL models using training scripts

### Issue: "CUDA out of memory"
**Solution:** Use CPU mode or reduce batch size

### Issue: "Rate limit exceeded"
**Solution:** Wait or increase RATE_LIMIT_REQUESTS in .env

### Issue: "TA-Lib import error"
**Solution:** Install TA-Lib system library (see installation section)

---

## üìù License

MIT License - See LICENSE file for details

---

## ü§ù Contributing

This is a technical test project. For the full product integration, the prediction agent will be connected to an executor via MCP.

---

**Built with ‚ù§Ô∏è for edge deployment**
